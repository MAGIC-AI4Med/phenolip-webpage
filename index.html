<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size:32px;
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 20px 0;
	}

	table.results th {
		background-color: #f2f2f2;
		padding: 10px;
		border: 1px solid #ddd;
		font-weight: 600;
		text-align: left;
	}

	table.results td {
		padding: 8px;
		border: 1px solid #ddd;
	}

	table.results tr:nth-child(even) {
		background-color: #f9f9f9;
	}

	.best-result {
		background-color: #d4edda !important;
		font-weight: bold;
	}

	.highlight-box {
		background-color: #fff3cd;
		border-left: 4px solid #ffc107;
		padding: 15px;
		margin: 20px 0;
	}
</style>

<html>
<head>
	<title>PhenoLIP: Phenotype Knowledge-Enhanced Vision-Language Pretraining</title>
	<meta property="og:image" content="https://magic-ai4med.github.io/phenolip-project-page/resources/overview.png"/>
	<meta property="og:title" content="PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision–Language Pretraining" />
	<meta property="og:description" content=" A novel framework that incorporates structured phenotype knowledge into medical vision-language pretraining, achieving state-of-the-art performance on phenotype recognition tasks." />
</head>

<body>
	<br>
	<center>
		<span style="font-size:38px; font-weight:400">PhenoLIP: Integrating Phenotype Ontology Knowledge<br>into Medical Vision–Language Pretraining</span>
		<table align=center width=900px>
			<tr>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px"><a href="https://github.com/LiangChengBupt">Cheng Liang<sup>1,2</sup></a></span>
					</center>
				</td>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px">Chaoyi Wu<sup>1</sup></span>
					</center>
				</td>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px">Weike Zhao<sup>1,2</sup></span>
					</center>
				</td>
			</tr>
			<tr>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px">Ya Zhang<sup>1,2</sup></span>
					</center>
				</td>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px">Yanfeng Wang<sup>1,2</sup></span>
					</center>
				</td>
				<td align=center width=150px>
					<center>
						<span style="font-size:22px">Weidi Xie<sup>1,2</sup></span>
					</center>
				</td>
			</tr>
		</table>

		<table align=center width=900px>
			<tr>
				<td align=center width=450px>
					<center>
						<span style="font-size:20px"><sup>1</sup>School of Artificial Intelligence, Shanghai Jiao Tong University</span>
					</center>
				</td>
				<td align=center width=450px>
					<center>
						<span style="font-size:20px"><sup>2</sup>Shanghai Artificial Intelligence Laboratory</span>
					</center>
				</td>
			</tr>
		</table>

		<table align=center width=400px>
			<tr>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><a href='#'>[Paper]</a></span>
					</center>
				</td>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><a href='https://github.com/MAGIC-AI4Med/PhenoLIP'>[GitHub]</a></span>
					</center>
				</td>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><a href='#'>[Dataset]</a></span>
					</center>
				</td>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><a href='#'>[Demo]</a></span>
					</center>
				</td>
			</tr>
		</table>


		<table align=center width=300px style="margin-top:10px;">
			<tr>
				<td align=center>
					<span style="font-size:22px; font-weight:500; color:#d9534f;">In Submission</span>
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=1000px style="margin-top:30px;">
			<tr>
				<td width=1000px>
					<center>
						<img class="rounded" style="width:1000px" src="./resources/overview.png"/>
					</center>
				</td>
			</tr>
			<tr>
				<td style="padding-top:10px;">
					<center>
						<span style="font-size:16px; font-style:italic;">
							Overview of PhenoKG and PhenoLIP: <b>(a)</b> PhenoKG organizes diverse anatomical systems hierarchically;
							<b>(b)</b> Unified multimodal integration aligning phenotype images, descriptions, and ontology knowledge;
							<b>(c)</b> PhenoLIP framework with knowledge-enhanced vision–language pretraining via distillation.
						</span>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td style="text-align:justify;">
				Recent progress in large-scale CLIP-like vision-language models (VLMs) has greatly advanced medical image analysis.
				However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies.
				To address this gap, we construct <b>PhenoKG</b>, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes.
				Building upon PhenoKG, we propose <b>PhenoLIP</b>, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process.
				We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective.
				To support evaluation, we further introduce <b>PhenoBench</b>, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image-caption pairs covering more than 1,000 phenotypes.
				Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.
			</td>
		</tr>
	</table>

	<div class="highlight-box" align=center style="width:850px; margin-left:auto; margin-right:auto;">
		<table width=100%>
			<tr>
				<td width=25% align=center>
					<span style="font-size:36px; font-weight:bold; color:#0066cc;">524K+</span><br>
					<span style="font-size:16px;">Image-Text Pairs</span>
				</td>
				<td width=25% align=center>
					<span style="font-size:36px; font-weight:bold; color:#0066cc;">3,000+</span><br>
					<span style="font-size:16px;">Phenotypes</span>
				</td>
				<td width=25% align=center>
					<span style="font-size:36px; font-weight:bold; color:#0066cc;">7,800+</span><br>
					<span style="font-size:16px;">Benchmark Pairs</span>
				</td>
				<td width=25% align=center>
					<span style="font-size:36px; font-weight:bold; color:#28a745;">36.56%</span><br>
					<span style="font-size:16px;">Zero-Shot Accuracy</span>
				</td>
			</tr>
		</table>
	</div>

	<br>
	<hr>

	<center><h1>Method</h1></center>

	<table align=center width=900px>
		<tr>
			<td align=center width=900px>
				<center>
					<img class="rounded" style="width:850px" src="./resources/pipeline.png"/>
				</center>
			</td>
		</tr>
		<tr>
			<td style="padding-top:10px;">
				<center>
					<span style="font-size:16px; font-style:italic;">
						PhenoLIP pipeline: Two-stage training process incorporating structured phenotype knowledge into vision-language pretraining through knowledge distillation.
					</span>
				</center>
			</td>
		</tr>
	</table>

	<table align=center width=850px style="margin-top:30px;">
		<tr>
			<td style="text-align:justify;">
				<h2>PhenoKG Construction</h2>
				We construct PhenoKG, a large-scale phenotype-centric multimodal knowledge graph, by integrating:
				<ul>
					<li><b>Image-text pairs:</b> Over 524K high-quality medical images with detailed phenotype descriptions</li>
					<li><b>Phenotype ontology:</b> Structured knowledge from Human Phenotype Ontology (HPO) covering 3,000+ phenotypes</li>
					<li><b>Hierarchical organization:</b> Multi-level anatomical system categorization for systematic knowledge representation</li>
				</ul>

				<h2>PhenoLIP Framework</h2>
				Our two-stage pretraining approach:
				<ol>
					<li><b>Stage 1 - Knowledge Embedding:</b> Learn a structured phenotype embedding space from textual ontology data using language models</li>
					<li><b>Stage 2 - Knowledge Distillation:</b> Distill the learned structured knowledge into vision-language pretraining via teacher-guided objectives</li>
				</ol>

				<h2>PhenoBench Benchmark</h2>
				We introduce an expert-verified benchmark with:
				<ul>
					<li>7,800+ carefully curated image-caption pairs</li>
					<li>1,000+ phenotypes for comprehensive evaluation</li>
					<li>Tasks: zero-shot classification and cross-modal retrieval</li>
				</ul>
			</td>
		</tr>
	</table>

	<hr>

	<center><h1>Results</h1></center>

	<table align=center width=850px>
		<tr>
			<td>
				<h2>Zero-Shot Phenotype Classification</h2>
				<table class="results">
					<tr>
						<th>Method</th>
						<th colspan="2">Encoder</th>
						<th colspan="7">Accuracy</th>
					</tr>
					<tr>
						<th></th>
						<th>Vision</th>
						<th>Text</th>
						<th>Dermatology</th>
						<th>Pathology</th>
						<th>Radiology</th>
						<th>Hematology</th>
						<th>Histology</th>
						<th>Ophthalmology</th>
						<th>PhenoBench</th>
						<th>Average</th>
					</tr>
					<tr>
						<td colspan="10" style="background-color:#f2f2f2; font-weight:600; text-align:center;">General VLMs</td>
					</tr>
					<tr>
						<td>OpenCLIP</td>
						<td>ViT-B</td>
						<td>GPT2</td>
						<td>14.77</td>
						<td>33.33</td>
						<td>29.75</td>
						<td><u>23.00</u></td>
						<td>4.52</td>
						<td>19.43</td>
						<td>2.26</td>
						<td>18.15</td>
					</tr>
					<tr>
						<td>SigLIP2</td>
						<td>So400m</td>
						<td>SigLIP64</td>
						<td>11.31</td>
						<td>20.00</td>
						<td>4.95</td>
						<td>8.30</td>
						<td><b>9.00</b></td>
						<td>23.00</td>
						<td>0.25</td>
						<td>10.97</td>
					</tr>
					<tr>
						<td>CoCa</td>
						<td>ViT-B</td>
						<td>GPT2</td>
						<td>12.63</td>
						<td>33.00</td>
						<td>25.87</td>
						<td>6.55</td>
						<td>3.39</td>
						<td>20.55</td>
						<td>1.38</td>
						<td>14.77</td>
					</tr>
					<tr>
						<td colspan="10" style="background-color:#f2f2f2; font-weight:600; text-align:center;">Biomedical VLMs</td>
					</tr>
					<tr>
						<td>PMC-CLIP</td>
						<td>ResNet50</td>
						<td>PubmedBert</td>
						<td>41.35</td>
						<td>45.12</td>
						<td>29.10</td>
						<td>21.50</td>
						<td>5.68</td>
						<td><u>43.75</u></td>
						<td>7.50</td>
						<td>27.71</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>ViT-B</td>
						<td>PubmedBert</td>
						<td>47.59</td>
						<td>42.87</td>
						<td>28.47</td>
						<td>20.40</td>
						<td>4.23</td>
						<td>40.18</td>
						<td><u>8.15</u></td>
						<td>27.41</td>
					</tr>
					<tr>
						<td>BIOMEDICA</td>
						<td>ViT-L</td>
						<td>GPT2</td>
						<td><b>56.76</b></td>
						<td><u>57.40</u></td>
						<td><u>37.72</u></td>
						<td>9.35</td>
						<td>5.40</td>
						<td>26.15</td>
						<td>6.69</td>
						<td><u>28.50</u></td>
					</tr>
					<tr class="best-result">
						<td><b>PhenoLIP</b></td>
						<td>ViT-B</td>
						<td>PubmedBert</td>
						<td style="font-weight:normal;"><u>55.08</u></td>
						<td><b>58.33</b></td>
						<td><b>49.03</b></td>
						<td><b>23.24</b></td>
						<td style="font-weight:normal;"><u>7.87</u></td>
						<td><b>51.63</b></td>
						<td><b>10.76</b></td>
						<td><b>36.56</b></td>
					</tr>
				</table>
				<p style="margin-top:15px;"><b>Key Finding:</b> PhenoLIP achieves an average accuracy of 36.56%, demonstrating strong generalization across diverse phenotype recognition tasks.</p>
			</td>
		</tr>

		<tr>
			<td style="padding-top:30px;">
				<h2>Cross-Modal Retrieval on PhenoBench</h2>
				<table class="results">
					<tr>
						<th>Method</th>
						<th colspan="2">Encoder</th>
						<th colspan="2">I2T</th>
						<th colspan="2">T2I</th>
						<th colspan="2">I2P</th>
						<th colspan="2">P2I</th>
					</tr>
					<tr>
						<th></th>
						<th>Vision</th>
						<th>Text</th>
						<th>R@10</th>
						<th>R@50</th>
						<th>R@10</th>
						<th>R@50</th>
						<th>R@10</th>
						<th>R@50</th>
						<th>R@10</th>
						<th>R@50</th>
					</tr>
					<tr>
						<td colspan="11" style="background-color:#f2f2f2; font-weight:600; text-align:center;">General VLMs</td>
					</tr>
					<tr>
						<td>OpenCLIP</td>
						<td>ViT-B</td>
						<td>GPT2</td>
						<td>10.72</td>
						<td>24.25</td>
						<td>10.08</td>
						<td>22.95</td>
						<td>2.88</td>
						<td>8.34</td>
						<td>2.79</td>
						<td>8.24</td>
					</tr>
					<tr>
						<td>SigLIP2</td>
						<td>So400m</td>
						<td>SigLIP64</td>
						<td>14.02</td>
						<td>28.16</td>
						<td>10.33</td>
						<td>23.44</td>
						<td>0.31</td>
						<td>0.76</td>
						<td>0.08</td>
						<td>0.62</td>
					</tr>
					<tr>
						<td>CoCa</td>
						<td>ViT-B</td>
						<td>GPT2</td>
						<td>9.27</td>
						<td>22.16</td>
						<td>7.57</td>
						<td>18.93</td>
						<td>2.02</td>
						<td>6.42</td>
						<td>1.82</td>
						<td>5.59</td>
					</tr>
					<tr>
						<td colspan="11" style="background-color:#f2f2f2; font-weight:600; text-align:center;">Biomedical VLMs</td>
					</tr>
					<tr>
						<td>PMC-CLIP</td>
						<td>ViT-L</td>
						<td>PubmedBert</td>
						<td>40.00</td>
						<td>64.82</td>
						<td>36.68</td>
						<td>61.83</td>
						<td>7.22</td>
						<td>23.44</td>
						<td>6.64</td>
						<td>18.92</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>ViT-B</td>
						<td>PubmedBert</td>
						<td>32.91</td>
						<td>56.63</td>
						<td>32.43</td>
						<td>56.08</td>
						<td>3.71</td>
						<td>13.38</td>
						<td>3.77</td>
						<td>12.17</td>
					</tr>
					<tr>
						<td>BIOMEDICA</td>
						<td>ViT-L</td>
						<td>GPT2</td>
						<td>40.51</td>
						<td>66.28</td>
						<td>40.03</td>
						<td>67.38</td>
						<td>8.12</td>
						<td>25.27</td>
						<td>6.82</td>
						<td>19.60</td>
					</tr>
					<tr class="best-result">
						<td><b>PhenoLIP</b></td>
						<td>ViT-B</td>
						<td>PubmedBert</td>
						<td><b>63.30</b></td>
						<td><b>81.92</b></td>
						<td><b>66.61</b></td>
						<td><b>87.68</b></td>
						<td><b>13.84</b></td>
						<td><b>36.88</b></td>
						<td><b>12.77</b></td>
						<td><b>31.30</b></td>
					</tr>
				</table>
				<p style="margin-top:15px;"><b>Caption:</b> Cross-modal retrieval results on PhenoBench (7,819 images, 1,187 phenotypes). I2T represents image-to-text retrieval and T2I represents text-to-image retrieval. I2P represents image-to-phenotype retrieval and P2I represents phenotype-to-image retrieval.</p>
			</td>
		</tr>

		<tr>
			<td style="padding-top:30px;">
				<h2>Rare Facial Phenotype Identification on Face2Gene</h2>
				<table class="results">
					<tr>
						<th>Method</th>
						<th colspan="3">Retrieval Metrics (%)</th>
						<th colspan="3">Matching Metrics (%)</th>
					</tr>
					<tr>
						<th></th>
						<th>R@5</th>
						<th>R@10</th>
						<th>R@50</th>
						<th>Precision</th>
						<th>Recall</th>
						<th>F1</th>
					</tr>
					<tr>
						<td colspan="7" style="background-color:#f2f2f2; font-weight:600;">General VLMs</td>
					</tr>
					<tr>
						<td>OpenCLIP</td>
						<td><u>6.97</u></td>
						<td><u>11.94</u></td>
						<td>39.80</td>
						<td><u>1.63</u></td>
						<td>2.56</td>
						<td><u>1.81</u></td>
					</tr>
					<tr>
						<td>SigLIP2</td>
						<td>6.85</td>
						<td>11.80</td>
						<td>39.50</td>
						<td>1.50</td>
						<td>2.10</td>
						<td>1.75</td>
					</tr>
					<tr>
						<td>CoCa</td>
						<td>4.48</td>
						<td>7.46</td>
						<td>38.31</td>
						<td>1.04</td>
						<td>2.21</td>
						<td>1.25</td>
					</tr>
					<tr>
						<td colspan="7" style="background-color:#f2f2f2; font-weight:600;">Biomedical VLMs</td>
					</tr>
					<tr>
						<td>PMC-CLIP</td>
						<td>6.75</td>
						<td>11.60</td>
						<td>49.50</td>
						<td>1.65</td>
						<td>3.20</td>
						<td>2.17</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>6.84</td>
						<td>11.73</td>
						<td><u>49.84</u></td>
						<td><u>1.80</u></td>
						<td><u>3.77</u></td>
						<td><u>2.22</u></td>
					</tr>
					<tr>
						<td>BIOMEDICA</td>
						<td>3.48</td>
						<td>7.46</td>
						<td>43.28</td>
						<td>1.10</td>
						<td>2.37</td>
						<td>1.36</td>
					</tr>
					<tr class="best-result">
						<td><b>PhenoLIP</b></td>
						<td><b>7.49</b></td>
						<td><b>12.05</b></td>
						<td><b>55.05</b></td>
						<td><b>2.08</b></td>
						<td><b>4.56</b></td>
						<td><b>2.62</b></td>
					</tr>
				</table>
				<p style="margin-top:15px;"><b>Key Finding:</b> On the challenging Face2Gene dataset with 321 images and 993 candidate phenotypes, PhenoLIP achieves the highest F1-score of 2.62%, demonstrating enhanced ability to capture subtle visual signatures of rare diseases.</p>
			</td>
		</tr>

		<tr>
			<td style="padding-top:30px;">
				<h2>Linear Probing Performance</h2>
				<p style="margin-bottom:15px;">Benchmarking results on Linear Evaluation (Acc) across different data ratios:</p>
				<div style="overflow-x:auto;">
				<table class="results" style="font-size:14px;">
					<tr>
						<th rowspan="2">Method</th>
						<th colspan="3">RSNA</th>
						<th colspan="3">BreastMNIST</th>
						<th colspan="3">ChestMNIST</th>
						<th colspan="3">DermaMNIST</th>
						<th colspan="3">OCTMNIST</th>
						<th colspan="3">RetinaMNIST</th>
						<th colspan="3">HAM10000</th>
					</tr>
					<tr>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
						<th>1%</th><th>10%</th><th>100%</th>
					</tr>
					<tr>
						<td colspan="22" style="background-color:#f2f2f2; font-weight:600; text-align:center;">General VLMs</td>
					</tr>
					<tr>
						<td>OpenCLIP</td>
						<td>77.07</td><td>79.52</td><td>81.61</td>
						<td>32.05</td><td>73.72</td><td>83.33</td>
						<td>51.31</td><td>52.88</td><td>53.69</td>
						<td>69.53</td><td>77.51</td><td><u>83.74</u></td>
						<td>67.40</td><td>73.20</td><td>73.10</td>
						<td>43.50</td><td>55.00</td><td><u>61.75</u></td>
						<td>52.81</td><td><u>68.65</u></td><td><u>77.23</u></td>
					</tr>
					<tr>
						<td>SigLIP2</td>
						<td>54.55</td><td>68.90</td><td>77.50</td>
						<td>32.50</td><td>73.50</td><td>83.00</td>
						<td>51.05</td><td>52.50</td><td>53.80</td>
						<td>69.00</td><td>77.10</td><td>83.50</td>
						<td>67.00</td><td>72.90</td><td>72.50</td>
						<td>43.80</td><td>54.50</td><td>61.00</td>
						<td>53.24</td><td>66.67</td><td>72.57</td>
					</tr>
					<tr>
						<td>CoCa</td>
						<td>77.29</td><td>79.59</td><td>81.36</td>
						<td>36.54</td><td>78.21</td><td>82.05</td>
						<td>51.62</td><td>52.85</td><td>53.48</td>
						<td>68.73</td><td>75.26</td><td>81.40</td>
						<td>43.75</td><td>55.75</td><td>59.50</td>
						<td><b>52.24</b></td><td><u>56.92</u></td><td>58.37</td>
						<td>55.45</td><td>66.67</td><td>72.94</td>
					</tr>
					<tr>
						<td colspan="22" style="background-color:#f2f2f2; font-weight:600; text-align:center;">Biomedical VLMs</td>
					</tr>
					<tr>
						<td>PMC-CLIP</td>
						<td><u>82.49</u></td><td><b>83.74</b></td><td>83.76</td>
						<td>26.92</td><td>75.00</td><td>82.05</td>
						<td>49.50</td><td>53.70</td><td><b>55.19</b></td>
						<td>69.23</td><td>75.26</td><td>80.55</td>
						<td><b>76.20</b></td><td>69.90</td><td>71.60</td>
						<td><u>45.00</u></td><td>56.50</td><td>60.00</td>
						<td>54.13</td><td>64.36</td><td>75.25</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>81.99</td><td><u>83.66</u></td><td><u>83.99</u></td>
						<td><b>51.92</b></td><td><b>79.49</b></td><td>84.62</td>
						<td>51.31</td><td><u>54.46</u></td><td>55.03</td>
						<td>69.28</td><td>74.86</td><td>79.25</td>
						<td>68.20</td><td>73.80</td><td>74.10</td>
						<td>43.50</td><td>55.75</td><td>59.00</td>
						<td><u>59.74</u></td><td>65.02</td><td>72.61</td>
					</tr>
					<tr>
						<td>BIOMEDICA</td>
						<td>80.09</td><td>82.94</td><td>83.44</td>
						<td>37.82</td><td><u>77.56</u></td><td><u>85.90</u></td>
						<td><u>51.99</u></td><td>53.57</td><td>54.45</td>
						<td><b>70.87</b></td><td><b>79.10</b></td><td><b>85.14</b></td>
						<td>61.40</td><td><u>76.40</u></td><td><u>76.70</u></td>
						<td>43.50</td><td>55.50</td><td><b>63.50</b></td>
						<td>57.76</td><td><b>69.31</b></td><td>73.60</td>
					</tr>
					<tr class="best-result">
						<td><b>PhenoLIP</b></td>
						<td><b>83.06</b></td><td>83.36</td><td><b>84.11</b></td>
						<td style="font-weight:normal;"><u>49.36</u></td><td>74.36</td><td><b>86.54</b></td>
						<td><b>52.65</b></td><td><b>54.71</b></td><td style="font-weight:normal;"><u>55.15</u></td>
						<td style="font-weight:normal;"><u>69.93</u></td><td style="font-weight:normal;"><u>75.86</u></td><td style="font-weight:normal;"><u>83.74</u></td>
						<td style="font-weight:normal;"><u>73.90</u></td><td><b>78.30</b></td><td><b>77.10</b></td>
						<td>43.50</td><td><b>57.75</b></td><td style="font-weight:normal;"><u>61.75</u></td>
						<td><b>62.71</b></td><td><b>69.31</b></td><td><b>77.89</b></td>
					</tr>
				</table>
				</div>
				<p style="margin-top:15px;"><b>Caption:</b> Benchmarking results on Linear Evaluation (Acc) across different data ratios. The best-performing model for each setting is in <b>bold</b>, and the second-best is <u>underlined</u>.</p>
			</td>
		</tr>

		<tr>
			<td style="padding-top:30px;">
				<h2>Ablation Studies</h2>
				<p style="margin-bottom:15px;">Impact of different components on model performance:</p>
				<table class="results">
					<tr>
						<th colspan="2">Encoder</th>
						<th>Knowledge Distillation</th>
						<th>Data Curation</th>
						<th colspan="2">I2T</th>
						<th colspan="2">T2I</th>
						<th>Classification Acc</th>
					</tr>
					<tr>
						<th>Vision</th>
						<th>Text</th>
						<th></th>
						<th></th>
						<th>R@10</th>
						<th>R@50</th>
						<th>R@10</th>
						<th>R@50</th>
						<th></th>
					</tr>
					<tr>
						<td>Scratch</td>
						<td>KB</td>
						<td>-</td>
						<td>-</td>
						<td>28.15</td>
						<td>45.33</td>
						<td>31.04</td>
						<td>49.81</td>
						<td>2.13</td>
					</tr>
					<tr>
						<td>CLIP</td>
						<td>KB</td>
						<td>-</td>
						<td>-</td>
						<td>47.20</td>
						<td>68.91</td>
						<td>51.72</td>
						<td>73.05</td>
						<td>6.44</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>KB</td>
						<td>-</td>
						<td>-</td>
						<td>50.11</td>
						<td>71.22</td>
						<td>53.68</td>
						<td>75.90</td>
						<td>7.02</td>
					</tr>
					<tr>
						<td>CLIP</td>
						<td>PMB</td>
						<td>-</td>
						<td>-</td>
						<td>49.53</td>
						<td>70.88</td>
						<td>54.88</td>
						<td>78.14</td>
						<td>6.95</td>
					</tr>
					<tr>
						<td>BiomedCLIP</td>
						<td>PMB</td>
						<td>-</td>
						<td>-</td>
						<td>53.42</td>
						<td>74.10</td>
						<td>58.03</td>
						<td>81.25</td>
						<td>7.81</td>
					</tr>
					<tr>
						<td>CLIP</td>
						<td>PMB</td>
						<td>&#10003;</td>
						<td>-</td>
						<td>58.91</td>
						<td>78.54</td>
						<td>62.19</td>
						<td>84.33</td>
						<td>9.57</td>
					</tr>
					<tr>
						<td>CLIP</td>
						<td>PMB</td>
						<td>&#10003;</td>
						<td>&#10003;</td>
						<td>60.13</td>
						<td>79.62</td>
						<td>63.55</td>
						<td>85.18</td>
						<td>9.98</td>
					</tr>
					<tr class="best-result">
						<td><b>BiomedCLIP</b></td>
						<td><b>PMB</b></td>
						<td><b>&#10003;</b></td>
						<td><b>&#10003;</b></td>
						<td><b>63.30</b></td>
						<td><b>81.92</b></td>
						<td><b>66.61</b></td>
						<td><b>87.68</b></td>
						<td><b>10.76</b></td>
					</tr>
				</table>
				<p style="margin-top:15px;"><b>Key Findings:</b> Domain-specific pre-trained encoders provide strong baselines. Knowledge distillation significantly improves performance by injecting structured ontological knowledge. Data curation (subfigure detection and LLM-based caption refinement) consistently enhances model performance. The full model combining all components achieves the best results. (KB: Knowledge-enhanced BERT; PMB: PubmedBERT)</p>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=550px>
		<center><h1>Paper and Citation</h1></center>
		<tr>
			<td width=300px align=center>
				<span style="font-size:16pt">
					C. Liang, C. Wu, W. Zhao, Y. Zhang, Y. Wang, W. Xie<br>
					<b>PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision–Language Pretraining</b><br>
					In Submission, 2026.<br>
					<br>
					<span style="font-size:14pt">
						<a href="#">[Paper]</a> &nbsp;
						<a href="https://github.com/MAGIC-AI4Med/PhenoLIP">[Code]</a> &nbsp;
						<a href="./resources/bibtex.txt">[BibTeX]</a>
					</span>
				</span>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This webpage template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project. The code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>
